{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from category_encoders import TargetEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "from xgboost.callback import EarlyStopping\n",
        "from lightgbm import early_stopping\n",
        "\n",
        "\n",
        "# Load Data\n",
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')\n",
        "sample_submission = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "# Haversine function\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371\n",
        "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "    a = (np.sin(dlat / 2.0)**2\n",
        "         + np.cos(lat1)*np.cos(lat2)*np.sin(dlon / 2.0)**2)\n",
        "    c = 2 * np.arcsin(np.sqrt(a))\n",
        "    return R * c\n",
        "\n",
        "def preprocess_data(data, is_train=True, fraud_counts=None):\n",
        "    if is_train:\n",
        "        fraud_counts = data.groupby(['cc_num', 'is_fraud']).size().unstack(fill_value=0).reset_index()\n",
        "        fraud_counts.columns = ['cc_num', 'is_fraud_0_count', 'is_fraud_1_count']\n",
        "        fraud_counts['fraud_score'] = (fraud_counts['is_fraud_0_count'] * 10) - (fraud_counts['is_fraud_1_count'] * 50)\n",
        "\n",
        "    data = data.merge(fraud_counts, on='cc_num', how='left')\n",
        "    data['trans_datetime'] = pd.to_datetime(data['trans_date'] + ' ' + data['trans_time'])\n",
        "    data['dob'] = pd.to_datetime(data['dob'], errors='coerce')\n",
        "    data['age'] = (data['trans_datetime'] - data['dob']).dt.days / 365.25\n",
        "    data['hour'] = data['trans_datetime'].dt.hour\n",
        "    data['day'] = data['trans_datetime'].dt.day\n",
        "    data['month'] = data['trans_datetime'].dt.month\n",
        "    data['weekday'] = data['trans_datetime'].dt.weekday\n",
        "    data['haversine_distance'] = haversine(data['lat'], data['long'], data['merch_lat'], data['merch_long'])\n",
        "\n",
        "    features = [\n",
        "        'amt', 'gender', 'category', 'job', 'state', 'city_pop',\n",
        "        'hour', 'day', 'month', 'weekday',\n",
        "        'age', 'haversine_distance'\n",
        "    ]\n",
        "\n",
        "    if is_train:\n",
        "        features += ['is_fraud']\n",
        "\n",
        "    data = data[features]\n",
        "\n",
        "    # Gender map\n",
        "    gender_map = {'F': 0, 'M': 1}\n",
        "    data['gender'] = data['gender'].map(gender_map)\n",
        "\n",
        "    # Target Encoding or Label Encoding for categorical columns\n",
        "    categorical_cols = ['category', 'job', 'state']\n",
        "    for col in categorical_cols:\n",
        "        data[col] = data[col].astype(str)\n",
        "        le = LabelEncoder()\n",
        "        data[col] = le.fit_transform(data[col])\n",
        "\n",
        "    # Impute missing values\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    data = pd.DataFrame(imputer.fit_transform(data), columns=features)\n",
        "\n",
        "    if is_train:\n",
        "        return data, fraud_counts\n",
        "    else:\n",
        "        return data\n",
        "\n",
        "\n",
        "# Preprocess train\n",
        "train_data, fraud_counts = preprocess_data(train_data, is_train=True)\n",
        "X = train_data.drop('is_fraud', axis=1)\n",
        "y = train_data['is_fraud']\n",
        "\n",
        "# Preprocess test\n",
        "test_data = preprocess_data(test_data, is_train=False, fraud_counts=fraud_counts)\n",
        "\n",
        "# Ensure test has all columns in X\n",
        "missing_cols = set(X.columns) - set(test_data.columns)\n",
        "for col in missing_cols:\n",
        "    test_data[col] = 0\n",
        "test_data = test_data[X.columns]\n",
        "\n",
        "# Categorical columns for target encoding\n",
        "# After imputation and mapping, we consider original categorical features\n",
        "cat_cols = ['category', 'job', 'state']\n",
        "\n",
        "# Apply Target Encoding on cat cols\n",
        "te = TargetEncoder(cols=cat_cols)\n",
        "te.fit(X[cat_cols], y)\n",
        "X[cat_cols] = te.transform(X[cat_cols])\n",
        "test_data[cat_cols] = te.transform(test_data[cat_cols])\n",
        "\n",
        "# StratifiedKFold for out-of-fold predictions\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Base models\n",
        "xgb_params = {\n",
        "    'random_state': 42,\n",
        "    'n_estimators': 300,\n",
        "    'use_label_encoder': False,\n",
        "    'eval_metric': 'logloss'\n",
        "}\n",
        "lgb_params = {\n",
        "    'random_state': 42,\n",
        "    'n_estimators': 300\n",
        "}\n",
        "cat_params = {\n",
        "    'random_state': 42,\n",
        "    'iterations': 300,\n",
        "    'verbose': False\n",
        "}\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(**xgb_params)\n",
        "lgb_model = lgb.LGBMClassifier(**lgb_params)\n",
        "cat_model = CatBoostClassifier(**cat_params)\n",
        "\n",
        "# Prepare arrays for out-of-fold predictions\n",
        "oof_xgb = np.zeros(len(X))\n",
        "oof_lgb = np.zeros(len(X))\n",
        "oof_cat = np.zeros(len(X))\n",
        "\n",
        "callbacks = [EarlyStopping(rounds=50, save_best=True, maximize=False)]\n",
        "\n",
        "# Train base models with OOF predictions\n",
        "for train_idx, val_idx in skf.split(X, y):\n",
        "    X_tr, X_val_ = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_tr, y_val_ = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "    # XGB\n",
        "    xgb_model.set_params(early_stopping_rounds=50)\n",
        "    xgb_model.fit(\n",
        "        X_tr, y_tr,\n",
        "        eval_set=[(X_val_, y_val_)],\n",
        "        verbose=False\n",
        "    )\n",
        "    oof_xgb[val_idx] = xgb_model.predict_proba(X_val_)[:, 1]\n",
        "\n",
        "    # LGB\n",
        "    callbacks = [early_stopping(stopping_rounds=50, verbose=False)]\n",
        "    lgb_model.fit(X_tr, y_tr, eval_set=[(X_val_, y_val_)], callbacks=callbacks)\n",
        "    oof_lgb[val_idx] = lgb_model.predict_proba(X_val_)[:, 1]\n",
        "\n",
        "    # Cat\n",
        "    cat_model.fit(X_tr, y_tr, eval_set=(X_val_, y_val_),\n",
        "                  early_stopping_rounds=50, use_best_model=True)\n",
        "    oof_cat[val_idx] = cat_model.predict_proba(X_val_)[:, 1]\n",
        "\n",
        "\n",
        "# Stack model input: OOF predictions as features\n",
        "stack_train = np.vstack([oof_xgb, oof_lgb, oof_cat]).T\n",
        "\n",
        "# Fit a meta-classifier (Logistic Regression) on the OOF predictions\n",
        "meta_model = LogisticRegression(random_state=42)\n",
        "meta_model.fit(stack_train, y)\n",
        "# Evaluate on a separate hold-out\n",
        "# Let's do a final hold-out just for evaluation\n",
        "X_train_, X_val_, y_train_, y_val_ = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Re-train base models on full X_train_ and predict on X_val_ for final evaluation\n",
        "xgb_model.fit(X_train_, y_train_, eval_set=[(X_val_, y_val_)], verbose=False) # Added eval_set\n",
        "lgb_model.fit(X_train_, y_train_)\n",
        "cat_model.fit(X_train_, y_train_)\n",
        "\n",
        "val_xgb = xgb_model.predict_proba(X_val_)[:,1]\n",
        "val_lgb = lgb_model.predict_proba(X_val_)[:,1]\n",
        "val_cat = cat_model.predict_proba(X_val_)[:,1]\n",
        "\n",
        "stack_val = np.vstack([val_xgb, val_lgb, val_cat]).T\n",
        "val_pred = (meta_model.predict_proba(stack_val)[:,1] > 0.5).astype(int)\n",
        "\n",
        "print(\"Classification Report (Validation):\")\n",
        "print(classification_report(y_val_, val_pred))\n",
        "print(\"Confusion Matrix (Validation):\")\n",
        "print(confusion_matrix(y_val_, val_pred))\n",
        "print(\"F1 Score (Validation):\", f1_score(y_val_, val_pred))\n",
        "\n",
        "# Disable early stopping for final training\n",
        "xgb_model.set_params(early_stopping_rounds=None)\n",
        "xgb_model.fit(X, y)\n",
        "\n",
        "lgb_model.fit(X, y)\n",
        "cat_model.fit(X, y)\n",
        "\n",
        "# Generate predictions for the test set\n",
        "test_xgb = xgb_model.predict_proba(test_data)[:, 1]\n",
        "test_lgb = lgb_model.predict_proba(test_data)[:, 1]\n",
        "test_cat = cat_model.predict_proba(test_data)[:, 1]\n",
        "\n",
        "# Stack predictions for meta-classifier\n",
        "test_stack = np.vstack([test_xgb, test_lgb, test_cat]).T\n",
        "test_pred = (meta_model.predict_proba(test_stack)[:, 1] > 0.5).astype(int)\n",
        "\n",
        "# Create submission file\n",
        "submission = pd.DataFrame({'id': sample_submission['id'], 'is_fraud': test_pred})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"Submission file created: submission.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV2pPhhAhBzS",
        "outputId": "399e82ca-3aef-4d99-fb7f-e5eb64520a22"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [03:46:28] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 33839, number of negative: 262723\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010591 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1403\n",
            "[LightGBM] [Info] Number of data points in the train set: 296562, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114104 -> initscore=-2.049486\n",
            "[LightGBM] [Info] Start training from score -2.049486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [03:47:14] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 33839, number of negative: 262723\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061726 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1402\n",
            "[LightGBM] [Info] Number of data points in the train set: 296562, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114104 -> initscore=-2.049486\n",
            "[LightGBM] [Info] Start training from score -2.049486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [03:48:00] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 33839, number of negative: 262723\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010648 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1403\n",
            "[LightGBM] [Info] Number of data points in the train set: 296562, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114104 -> initscore=-2.049486\n",
            "[LightGBM] [Info] Start training from score -2.049486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [03:48:49] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 33840, number of negative: 262723\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010845 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1403\n",
            "[LightGBM] [Info] Number of data points in the train set: 296563, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114107 -> initscore=-2.049457\n",
            "[LightGBM] [Info] Start training from score -2.049457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [03:49:35] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 33839, number of negative: 262724\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010752 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1402\n",
            "[LightGBM] [Info] Number of data points in the train set: 296563, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114104 -> initscore=-2.049490\n",
            "[LightGBM] [Info] Start training from score -2.049490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [03:50:23] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 33839, number of negative: 262723\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010773 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1403\n",
            "[LightGBM] [Info] Number of data points in the train set: 296562, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114104 -> initscore=-2.049486\n",
            "[LightGBM] [Info] Start training from score -2.049486\n",
            "Classification Report (Validation):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00     65681\n",
            "         1.0       0.99      0.97      0.98      8460\n",
            "\n",
            "    accuracy                           1.00     74141\n",
            "   macro avg       0.99      0.98      0.99     74141\n",
            "weighted avg       1.00      1.00      1.00     74141\n",
            "\n",
            "Confusion Matrix (Validation):\n",
            "[[65592    89]\n",
            " [  243  8217]]\n",
            "F1 Score (Validation): 0.9801980198019802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [03:51:08] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 42299, number of negative: 328404\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013553 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1403\n",
            "[LightGBM] [Info] Number of data points in the train set: 370703, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114105 -> initscore=-2.049481\n",
            "[LightGBM] [Info] Start training from score -2.049481\n",
            "Submission file created: submission.csv\n"
          ]
        }
      ]
    }
  ]
}